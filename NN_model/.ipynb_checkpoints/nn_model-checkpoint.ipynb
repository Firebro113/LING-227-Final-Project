{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8e6446-68b3-4967-b6e4-352b9c5bf5f8",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242caa2-9740-4d3a-a41e-78b057c7f015",
   "metadata": {},
   "source": [
    "## Data Extraction + W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d08a1e84-4b6c-49ec-afc8-c6b1f688a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316ab9f7-5e3a-4167-b300-a330c538b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = []\n",
    "\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            data.append((int(parts[0]), parts[1]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10bab59b-7efb-45d5-b239-74f51e0b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def greek_W2V(dim, quotes, window, min_count, workers):\n",
    "    '''\n",
    "    Create Word2Vec\n",
    "    '''\n",
    "    w2v_greek = Word2Vec(\n",
    "        sentences=quotes,\n",
    "        vector_size=dim,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers\n",
    "    )\n",
    "\n",
    "    w2v_greek.save(\"greek_word2vec.model\")\n",
    "    return w2v_greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b16e89a7-9e3e-475b-9723-b57a11187548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dubia_testset(data, chunk_size, w2v_model):\n",
    "    \"\"\"\n",
    "    Create Dubia Dataset Test Set\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    embeddings = []\n",
    "\n",
    "    for label, text in data:\n",
    "        if label != -1:\n",
    "            continue\n",
    "\n",
    "        # Split text into words and calculate number of chunks\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        text_chunks = [\n",
    "            \" \".join(words[i:i + chunk_size])\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "        \n",
    "        # Append results\n",
    "        chunks.extend(text_chunks)\n",
    "        embeddings.extend(chunk_embeddings)\n",
    "\n",
    "    return chunks, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "79e9eea9-29ff-49a7-bace-781af41573a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_text_testset(data, chunk_size, w2v_model, text_name):\n",
    "    \"\"\"\n",
    "    Prepares a test set for a single text in the dubia dataset by text name.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    embeddings = []\n",
    "\n",
    "    for label, text in data:\n",
    "        if label != -1:\n",
    "            continue\n",
    "\n",
    "        words = text.split()\n",
    "        if words[0] == text_name:\n",
    "            \n",
    "            # Remove the text name and first word (number) from the text body\n",
    "            text_body = \" \".join(words[1:])\n",
    "    \n",
    "            # Generate sliding window chunks\n",
    "            num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "            if num_chunks <= 0:\n",
    "                continue\n",
    "    \n",
    "            text_chunks = [\n",
    "                \" \".join(text_body.split()[i:i + chunk_size])\n",
    "                for i in range(num_chunks)\n",
    "            ]\n",
    "    \n",
    "            # Generate embeddings for each chunk\n",
    "            chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "    \n",
    "            # Append results\n",
    "            chunks.extend(text_chunks)\n",
    "            embeddings.extend(chunk_embeddings)\n",
    "    \n",
    "            embeddings = np.array(embeddings)\n",
    "            return chunks, embeddings\n",
    "\n",
    "    return chunks, np.array(embeddings)  # Fallback in case no matching text is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28ef28-d3bb-4f77-8c44-64cabd3b2ffd",
   "metadata": {},
   "source": [
    "## Basic NN with mean pooling of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70a7079c-630c-4a40-8638-a60dfd561803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_chunk(chunk, w2v_model):\n",
    "    \"\"\"\n",
    "    Converts a chunk of text into a fixed-size vector using mean pooling.\n",
    "    \"\"\"\n",
    "    words = chunk.split()\n",
    "    embedding_dim = w2v_model.vector_size\n",
    "    \n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    \n",
    "    if embeddings:\n",
    "        flattened_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        flattened_embedding = np.zeros(embedding_dim)\n",
    "    \n",
    "    return flattened_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1bd6e6e8-d966-42ab-a5ea-c2649c640061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_dataset(data, w2v_model, chunk_size, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Processes text data into sliding window chunks with embeddings,\n",
    "    splits the data into training, validation, and test sets, and randomizes it.\n",
    "    \"\"\"\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            \" \".join(words[i:i + chunk_size])\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        embeddings = [flatten_chunk(chunk, w2v_model) for chunk in chunks]\n",
    "        labels = [label] * len(embeddings)\n",
    "\n",
    "        # Split into train/val/test sets\n",
    "        X_temp, X_test_temp, y_temp, y_test_temp = train_test_split(\n",
    "            embeddings, labels, test_size=test_ratio, random_state=42\n",
    "        )\n",
    "        X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_ratio / (train_ratio + val_ratio), random_state=42\n",
    "        )\n",
    "\n",
    "        # Append to the main dataset\n",
    "        X_train.extend(X_train_temp)\n",
    "        y_train.extend(y_train_temp)\n",
    "        X_val.extend(X_val_temp)\n",
    "        y_val.extend(y_val_temp)\n",
    "        X_test.extend(X_test_temp)\n",
    "        y_test.extend(y_test_temp)\n",
    "\n",
    "    # Shuffle each dataset\n",
    "    train_data = list(zip(X_train, y_train))\n",
    "    val_data = list(zip(X_val, y_val))\n",
    "    test_data = list(zip(X_test, y_test))\n",
    "\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(val_data)\n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    # Unpack shuffled data\n",
    "    X_train, y_train = zip(*train_data)\n",
    "    X_val, y_val = zip(*val_data)\n",
    "    X_test, y_test = zip(*test_data)\n",
    "\n",
    "    return (\n",
    "        np.array(X_train), np.array(y_train),\n",
    "        np.array(X_val), np.array(y_val),\n",
    "        np.array(X_test), np.array(y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c02bd67-79cb-43eb-9cd0-a93bbf6a811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_nn(input_dim):\n",
    "    \"\"\"\n",
    "    Creates a simple feedforward neural network with input dimension specified.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865ef2e-57f4-4ceb-bff8-b77fa39cb00b",
   "metadata": {},
   "source": [
    "## Bsic NN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edfe23d7-c06d-487c-afd7-861e479f610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = load_data(\"./data_clean/combined.txt\")\n",
    "data_dubia = load_data(\"./data_clean/dubia.txt\")\n",
    "\n",
    "chunk_size = 25\n",
    "chunks = []\n",
    "for label, text in data_combined:\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words) - chunk_size + 1):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append((label, chunk))\n",
    "\n",
    "chunks_combined = [chunk.split() for _, chunk in chunks]\n",
    "dim = 100\n",
    "window = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "w2v_model = greek_W2V(dim, chunks_combined, window, min_count, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "affc2ff4-628f-471a-99ea-071ce7b7b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_dataset(data_combined, w2v_model, chunk_size, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f223bb4-1643-4280-a901-c63e2f9ea1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25430/25430 [==============================] - 15s 576us/step - loss: 0.3510 - accuracy: 0.8426 - val_loss: 0.2878 - val_accuracy: 0.8783\n",
      "Epoch 2/10\n",
      "25430/25430 [==============================] - 15s 590us/step - loss: 0.3144 - accuracy: 0.8625 - val_loss: 0.2657 - val_accuracy: 0.8875\n",
      "Epoch 3/10\n",
      "25430/25430 [==============================] - 15s 587us/step - loss: 0.3019 - accuracy: 0.8688 - val_loss: 0.2446 - val_accuracy: 0.8976\n",
      "Epoch 4/10\n",
      "25430/25430 [==============================] - 15s 594us/step - loss: 0.2947 - accuracy: 0.8726 - val_loss: 0.2445 - val_accuracy: 0.8970\n",
      "Epoch 5/10\n",
      "25430/25430 [==============================] - 15s 579us/step - loss: 0.2864 - accuracy: 0.8767 - val_loss: 0.2322 - val_accuracy: 0.9022\n",
      "Epoch 6/10\n",
      "25430/25430 [==============================] - 15s 581us/step - loss: 0.2818 - accuracy: 0.8789 - val_loss: 0.2353 - val_accuracy: 0.9014\n",
      "Epoch 7/10\n",
      "25430/25430 [==============================] - 15s 592us/step - loss: 0.2783 - accuracy: 0.8808 - val_loss: 0.2255 - val_accuracy: 0.9068\n",
      "Epoch 8/10\n",
      "25430/25430 [==============================] - 17s 676us/step - loss: 0.2739 - accuracy: 0.8830 - val_loss: 0.2211 - val_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "25430/25430 [==============================] - 15s 597us/step - loss: 0.2716 - accuracy: 0.8841 - val_loss: 0.2272 - val_accuracy: 0.9044\n",
      "Epoch 10/10\n",
      "25430/25430 [==============================] - 15s 589us/step - loss: 0.2693 - accuracy: 0.8853 - val_loss: 0.2168 - val_accuracy: 0.9134\n",
      "3180/3180 [==============================] - 1s 280us/step - loss: 0.2175 - accuracy: 0.9118\n",
      "Test Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "model = create_nn(input_dim)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24388f79-1428-4d22-8a5a-d8eefc9941c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623/1623 [==============================] - 0s 243us/step\n",
      "Predictions for dubia texts:\n",
      "[[0.9974538 ]\n",
      " [0.99908066]\n",
      " [0.9987066 ]\n",
      " ...\n",
      " [0.98489493]\n",
      " [0.9922986 ]\n",
      " [0.98907685]]\n"
     ]
    }
   ],
   "source": [
    "# Generate chunks and embeddings\n",
    "chunks, X_test_dubia = dubia_testset(data_dubia, chunk_size, w2v_model)\n",
    "\n",
    "# Get predictions for dubia texts\n",
    "dubia_predictions = model.predict(X_test_dubia)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions for dubia texts:\")\n",
    "print(dubia_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f04eb100-76cf-4a35-908b-a04cc050524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 305us/step\n",
      "0.28961602\n"
     ]
    }
   ],
   "source": [
    "# sorted_predictions = sorted(enumerate(dubia_predictions), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print(\"\\nSorted Predictions with Corresponding Chunks:\\n\")\n",
    "# for idx, prediction in sorted_predictions[:20]:\n",
    "#     print(f\"Prediction: {prediction[0]:.4f} | Chunk: {chunks[idx]}\")\n",
    "text_name = \"Definitions\"\n",
    "chunks, X_test_dubia = single_text_testset(data_dubia, chunk_size, w2v_model, text_name)\n",
    "\n",
    "dubia_predictions = model.predict(X_test_dubia)\n",
    "# print(\"\\nPredictions:\")\n",
    "# for chunk, prediction in zip(chunks, dubia_predictions):\n",
    "#     print(f\"Chunk: {chunk} | Prediction: {prediction[0]:.4f}\")\n",
    "average_prediction = np.mean(dubia_predictions)\n",
    "print(average_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
