{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8e6446-68b3-4967-b6e4-352b9c5bf5f8",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242caa2-9740-4d3a-a41e-78b057c7f015",
   "metadata": {},
   "source": [
    "## Data Extraction + W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08a1e84-4b6c-49ec-afc8-c6b1f688a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316ab9f7-5e3a-4167-b300-a330c538b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = []\n",
    "\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            data.append((int(parts[0]), parts[1]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bab59b-7efb-45d5-b239-74f51e0b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def greek_W2V(dim, quotes, window, min_count, workers):\n",
    "    '''\n",
    "    Create Word2Vec\n",
    "    '''\n",
    "    w2v_greek = Word2Vec(\n",
    "        sentences=quotes,\n",
    "        vector_size=dim,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers\n",
    "    )\n",
    "\n",
    "    w2v_greek.save(\"greek_word2vec.model\")\n",
    "    return w2v_greek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22fab7-160e-4b50-9412-cef16cdc883d",
   "metadata": {},
   "source": [
    "## Dubia and Testset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16e89a7-9e3e-475b-9723-b57a11187548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dubia_testset(data, chunk_size, w2v_model):\n",
    "    \"\"\"\n",
    "    Create Dubia Dataset Test Set\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    embeddings = []\n",
    "\n",
    "    for label, text in data:\n",
    "        if label != -1:\n",
    "            continue\n",
    "\n",
    "        # Split text into words and calculate number of chunks\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        text_chunks = [\n",
    "            \" \".join(words[i:i + chunk_size])\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "        \n",
    "        # Append results\n",
    "        chunks.extend(text_chunks)\n",
    "        embeddings.extend(chunk_embeddings)\n",
    "\n",
    "    return chunks, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e9eea9-29ff-49a7-bace-781af41573a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_text_testset(data, chunk_size, w2v_model, text_name):\n",
    "    \"\"\"\n",
    "    Prepares a test set for a single text in the dubia dataset by text name.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    embeddings = []\n",
    "\n",
    "    for label, text in data:\n",
    "        if label != -1:\n",
    "            continue\n",
    "\n",
    "        words = text.split()\n",
    "        if words[0] == text_name:\n",
    "            \n",
    "            # Remove the text name and first word (number) from the text body\n",
    "            text_body = \" \".join(words[1:])\n",
    "    \n",
    "            # Generate sliding window chunks\n",
    "            num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "            if num_chunks <= 0:\n",
    "                continue\n",
    "    \n",
    "            text_chunks = [\n",
    "                \" \".join(text_body.split()[i:i + chunk_size])\n",
    "                for i in range(num_chunks)\n",
    "            ]\n",
    "    \n",
    "            # Generate embeddings for each chunk\n",
    "            chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "    \n",
    "            # Append results\n",
    "            chunks.extend(text_chunks)\n",
    "            embeddings.extend(chunk_embeddings)\n",
    "    \n",
    "            embeddings = np.array(embeddings)\n",
    "            return chunks, embeddings\n",
    "\n",
    "    return chunks, np.array(embeddings)  # Fallback in case no matching text is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a874d796-723a-4aaa-ada3-0f07145a0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_texts_testset(data, chunk_size, w2v_model):\n",
    "    \"\"\"\n",
    "    Prepares a test set for all dubia texts, calculates predictions, and outputs mean scores.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        text_name = words[0]\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue  # Skip if the text is shorter than the chunk size\n",
    "\n",
    "        text_chunks = [\n",
    "            \" \".join(text_body.split()[i:i + chunk_size])\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "\n",
    "        # Convert embeddings to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Calculate the mean prediction\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28ef28-d3bb-4f77-8c44-64cabd3b2ffd",
   "metadata": {},
   "source": [
    "## Basic NN with mean pooling of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a7079c-630c-4a40-8638-a60dfd561803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_chunk(chunk, w2v_model):\n",
    "    \"\"\"\n",
    "    Converts a chunk of text into a fixed-size vector using mean pooling.\n",
    "    \"\"\"\n",
    "    words = chunk.split()\n",
    "    embedding_dim = w2v_model.vector_size\n",
    "    \n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    \n",
    "    if embeddings:\n",
    "        flattened_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        flattened_embedding = np.zeros(embedding_dim)\n",
    "    \n",
    "    return flattened_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd6e6e8-d966-42ab-a5ea-c2649c640061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_dataset(data, w2v_model, chunk_size, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Processes text data into sliding window chunks with embeddings,\n",
    "    splits the data into training, validation, and test sets, and randomizes it.\n",
    "    \"\"\"\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            \" \".join(words[i:i + chunk_size])\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        embeddings = [flatten_chunk(chunk, w2v_model) for chunk in chunks]\n",
    "        labels = [label] * len(embeddings)\n",
    "\n",
    "        # Split into train/val/test sets\n",
    "        X_temp, X_test_temp, y_temp, y_test_temp = train_test_split(\n",
    "            embeddings, labels, test_size=test_ratio, random_state=42\n",
    "        )\n",
    "        X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_ratio / (train_ratio + val_ratio), random_state=42\n",
    "        )\n",
    "\n",
    "        # Append to the main dataset\n",
    "        X_train.extend(X_train_temp)\n",
    "        y_train.extend(y_train_temp)\n",
    "        X_val.extend(X_val_temp)\n",
    "        y_val.extend(y_val_temp)\n",
    "        X_test.extend(X_test_temp)\n",
    "        y_test.extend(y_test_temp)\n",
    "\n",
    "    # Shuffle each dataset\n",
    "    train_data = list(zip(X_train, y_train))\n",
    "    val_data = list(zip(X_val, y_val))\n",
    "    test_data = list(zip(X_test, y_test))\n",
    "\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(val_data)\n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    # Unpack shuffled data\n",
    "    X_train, y_train = zip(*train_data)\n",
    "    X_val, y_val = zip(*val_data)\n",
    "    X_test, y_test = zip(*test_data)\n",
    "\n",
    "    return (\n",
    "        np.array(X_train), np.array(y_train),\n",
    "        np.array(X_val), np.array(y_val),\n",
    "        np.array(X_test), np.array(y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c02bd67-79cb-43eb-9cd0-a93bbf6a811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_nn(input_dim):\n",
    "    \"\"\"\n",
    "    Creates a simple feedforward neural network with input dimension specified.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865ef2e-57f4-4ceb-bff8-b77fa39cb00b",
   "metadata": {},
   "source": [
    "## Bsic NN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a66ab17-753c-48c1-8ac1-8dd3849fc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfe23d7-c06d-487c-afd7-861e479f610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = load_data(\"./data_clean/combined.txt\")\n",
    "data_dubia = load_data(\"./data_clean/dubia.txt\")\n",
    "\n",
    "chunk_size = 25\n",
    "dim = 100\n",
    "# chunks = []\n",
    "# for label, text in data_combined:\n",
    "#     words = text.split()\n",
    "#     for i in range(0, len(words) - chunk_size + 1):\n",
    "#         chunk = ' '.join(words[i:i + chunk_size])\n",
    "#         chunks.append((label, chunk))\n",
    "\n",
    "# chunks_combined = [chunk.split() for _, chunk in chunks]\n",
    "# dim = 100\n",
    "# window = 5\n",
    "# min_count = 1\n",
    "# workers = 4\n",
    "# w2v_model = greek_W2V(dim, chunks_combined, window, min_count, workers)\n",
    "w2v_model = Word2Vec.load(\"greek_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "affc2ff4-628f-471a-99ea-071ce7b7b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_dataset(data_combined, w2v_model, chunk_size, 0.8, 0.1, 0.1)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save datasets to a pickle file\n",
    "with open(f\"nn_datasets_{chunk_size}.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce8ff78d-584a-4524-a93f-e01b43295097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"nn_datasets_{chunk_size}.pkl\", \"rb\") as f:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f223bb4-1643-4280-a901-c63e2f9ea1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25304/25304 [==============================] - 16s 620us/step - loss: 0.1626 - accuracy: 0.9359 - val_loss: 0.0829 - val_accuracy: 0.9684\n",
      "Epoch 2/10\n",
      "25304/25304 [==============================] - 15s 611us/step - loss: 0.1321 - accuracy: 0.9491 - val_loss: 0.0714 - val_accuracy: 0.9742\n",
      "Epoch 3/10\n",
      "25304/25304 [==============================] - 15s 611us/step - loss: 0.1200 - accuracy: 0.9540 - val_loss: 0.0672 - val_accuracy: 0.9745\n",
      "Epoch 4/10\n",
      "25304/25304 [==============================] - 15s 610us/step - loss: 0.1147 - accuracy: 0.9561 - val_loss: 0.0925 - val_accuracy: 0.9628\n",
      "Epoch 5/10\n",
      "25304/25304 [==============================] - 15s 610us/step - loss: 0.1106 - accuracy: 0.9580 - val_loss: 0.0740 - val_accuracy: 0.9724\n",
      "Epoch 6/10\n",
      "25304/25304 [==============================] - 15s 612us/step - loss: 0.1063 - accuracy: 0.9596 - val_loss: 0.0677 - val_accuracy: 0.9752\n",
      "Epoch 7/10\n",
      "25304/25304 [==============================] - 16s 615us/step - loss: 0.1048 - accuracy: 0.9603 - val_loss: 0.0652 - val_accuracy: 0.9768\n",
      "Epoch 8/10\n",
      "25304/25304 [==============================] - 15s 608us/step - loss: 0.1016 - accuracy: 0.9615 - val_loss: 0.0524 - val_accuracy: 0.9815\n",
      "Epoch 9/10\n",
      "25304/25304 [==============================] - 15s 574us/step - loss: 0.0983 - accuracy: 0.9627 - val_loss: 0.0545 - val_accuracy: 0.9807\n",
      "Epoch 10/10\n",
      "25304/25304 [==============================] - 16s 622us/step - loss: 0.0967 - accuracy: 0.9633 - val_loss: 0.0499 - val_accuracy: 0.9826\n",
      "3165/3165 [==============================] - 1s 298us/step - loss: 0.0500 - accuracy: 0.9828\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "model = create_nn(input_dim)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0166c68a-0705-4800-a19c-76981d2fc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"nn_model_weights_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e4ed990-3e7c-46f4-89d2-9ba28204f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nn(input_dim)\n",
    "model.load_weights(f\"nn_model_weights_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24388f79-1428-4d22-8a5a-d8eefc9941c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Generate chunks and embeddings\n",
    "# chunks, X_test_dubia = dubia_testset(data_dubia, chunk_size, w2v_model)\n",
    "\n",
    "# # Get predictions for dubia texts\n",
    "# dubia_predictions = model.predict(X_test_dubia)\n",
    "\n",
    "# # Print predictions\n",
    "# print(\"Predictions for dubia texts:\")\n",
    "# print(dubia_predictions)\n",
    "# # sorted_predictions = sorted(enumerate(dubia_predictions), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # print(\"\\nSorted Predictions with Corresponding Chunks:\\n\")\n",
    "# # for idx, prediction in sorted_predictions[:20]:\n",
    "# #     print(f\"Prediction: {prediction[0]:.4f} | Chunk: {chunks[idx]}\")\n",
    "# text_name = \"Definitions\"\n",
    "# chunks, X_test_dubia = single_text_testset(data_dubia, chunk_size, w2v_model, text_name)\n",
    "\n",
    "# dubia_predictions = model.predict(X_test_dubia)\n",
    "# # print(\"\\nPredictions:\")\n",
    "# # for chunk, prediction in zip(chunks, dubia_predictions):\n",
    "# #     print(f\"Chunk: {chunk} | Prediction: {prediction[0]:.4f}\")\n",
    "# average_prediction = np.mean(dubia_predictions)\n",
    "# print(average_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0392f6d9-4655-4dc1-ab42-a359f7332bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 297us/step\n",
      "106/106 [==============================] - 0s 270us/step\n",
      "72/72 [==============================] - 0s 249us/step\n",
      "528/528 [==============================] - 0s 242us/step\n",
      "318/318 [==============================] - 0s 247us/step\n",
      "128/128 [==============================] - 0s 249us/step\n",
      "58/58 [==============================] - 0s 255us/step\n",
      "67/67 [==============================] - 0s 252us/step\n",
      "195/195 [==============================] - 0s 246us/step\n",
      "46/46 [==============================] - 0s 258us/step\n",
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.9468\n",
      "Theages: 0.9945\n",
      "Lovers: 0.9888\n",
      "Letters: 0.8625\n",
      "Alcibiades1: 0.9717\n",
      "Alcibiades2: 0.9234\n",
      "Definitions: 0.0923\n",
      "Hipparchus: 0.9058\n",
      "Epinomis: 0.9070\n",
      "Cleitophon: 0.9337\n"
     ]
    }
   ],
   "source": [
    "results = all_texts_testset(data_dubia, chunk_size, w2v_model)\n",
    "\n",
    "# Print the mean scores for each text\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    if mean_score is not None:\n",
    "        print(f\"{text_name}: {mean_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{text_name}: No valid chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345e9ff-b1bb-42e2-8116-f133f3047d7b",
   "metadata": {},
   "source": [
    "## RNN LSTM -> NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d11725-1f52-4ced-b3cf-2a91986db927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_lstm_datasets(data, chunk_size, w2v_model, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Prepares datasets for an LSTM model by processing text into sequences of embeddings and splitting into train/val/test.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            words[i:i + chunk_size]\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert each chunk into a sequence of embeddings\n",
    "        for chunk in chunks:\n",
    "            embeddings = [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            X.append(embeddings)\n",
    "            y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split into train, val, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(val_ratio + test_ratio), random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3adaa756-439b-43f0-9234-1cb5e6fe3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def lstm_datasets_percentage(data, chunk_size, w2v_model, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, sample_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Prepares datasets for an LSTM model by processing text into sequences of embeddings and splitting into train/val/test.\n",
    "    Samples a percentage of chunks from each text to reduce dataset size.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            words[i:i + chunk_size]\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Sample a percentage of the chunks\n",
    "        sampled_chunks = sample(chunks, int(len(chunks) * sample_ratio))\n",
    "\n",
    "        # Convert each chunk into a sequence of embeddings\n",
    "        for chunk in sampled_chunks:\n",
    "            embeddings = [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            X.append(embeddings)\n",
    "            y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split into train, val, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(val_ratio + test_ratio), random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb28c85-470a-450c-8beb-a4f6b195b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_lstm(input_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Creates a neural network with an LSTM layer to process sequential embeddings.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, input_shape=(input_dim, embedding_dim), return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f72f458-d757-40a0-8722-89201e67c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_texts_testset_lstm(data_dubia, chunk_size, w2v_model, model):\n",
    "    \"\"\"\n",
    "    Prepares and tests the LSTM model on all dubia texts, calculating mean predictions for each text.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data_dubia:\n",
    "        if label != -1:\n",
    "            continue  # Process only texts with label -1\n",
    "\n",
    "        words = text.split()\n",
    "        \n",
    "        text_name = words[0]\n",
    "\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            results[text_name] = None  # No valid chunks\n",
    "            continue\n",
    "\n",
    "        text_chunks = [\n",
    "            text_body.split()[i:i + chunk_size]\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert chunks into sequences of embeddings\n",
    "        chunk_embeddings = [\n",
    "            [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "\n",
    "        if not chunk_embeddings:\n",
    "            results[text_name] = None  # No valid embeddings\n",
    "            continue\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "        # Calculate mean prediction for the text\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b799b-a00a-467c-940e-ec29c550780a",
   "metadata": {},
   "source": [
    "## LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f99d2ad-58c1-4358-8ea0-b7477009a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "percentage = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772bc62a-f718-4b3e-9b5e-0eaa80b9bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_lstm_datasets(data_combined, chunk_size, w2v_model, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74c69137-8d2e-4cd0-8498-08c2e8003cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = lstm_datasets_percentage(data_combined, chunk_size, w2v_model, 0.8, 0.1, 0.1, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91ae3636-8c30-4014-a6f1-dee82aced6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save datasets to a pickle file\n",
    "with open(f\"lstm_datasets_{percentage}_{chunk_size}.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2d8af6e-6bf1-4b11-97cc-b330d28e6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"lstm_datasets_{percentage}_{chunk_size}.pkl\", \"rb\") as f:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13ff1528-535f-4e0c-9895-39744e9c04f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5078/5078 [==============================] - 91s 18ms/step - loss: 0.1442 - accuracy: 0.9450 - val_loss: 0.0546 - val_accuracy: 0.9807\n",
      "Epoch 2/5\n",
      "5078/5078 [==============================] - 92s 18ms/step - loss: 0.0503 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9855\n",
      "Epoch 3/5\n",
      "5078/5078 [==============================] - 107s 21ms/step - loss: 0.0310 - accuracy: 0.9906 - val_loss: 0.0264 - val_accuracy: 0.9912\n",
      "Epoch 4/5\n",
      "5078/5078 [==============================] - 93s 18ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.0185 - val_accuracy: 0.9946\n",
      "Epoch 5/5\n",
      "5078/5078 [==============================] - 94s 19ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.0182 - val_accuracy: 0.9944\n",
      "635/635 [==============================] - 4s 6ms/step - loss: 0.0200 - accuracy: 0.9945\n",
      "Test Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "model = create_lstm(chunk_size, w2v_model.vector_size)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "239857ad-4b87-4219-80a5-89dc44ea8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"lstm_model_weights_{percentage}_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90c482bd-a715-40a5-9591-e9f6d11c0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lstm(chunk_size, w2v_model.vector_size)\n",
    "\n",
    "model.load_weights(f\"lstm_model_weights_{percentage}_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "428d0564-ebc3-4d18-b9f2-b56b0aece94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.8927\n",
      "Theages: 0.9140\n",
      "Lovers: 0.9413\n",
      "Letters: 0.7361\n",
      "Alcibiades1: 0.9310\n",
      "Alcibiades2: 0.8200\n",
      "Definitions: 0.1641\n",
      "Hipparchus: 0.8869\n",
      "Epinomis: 0.9561\n",
      "Cleitophon: 0.8624\n"
     ]
    }
   ],
   "source": [
    "results = all_texts_testset_lstm(data_dubia, chunk_size, w2v_model, model)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    if mean_score is not None:\n",
    "        print(f\"{text_name}: {mean_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{text_name}: No valid chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64982a07-7338-42e2-9233-aeb8720a0e52",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48ba914f-a9e6-49e2-8337-1cff73a9ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Input, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def positional_encoding(max_position, embedding_dim):\n",
    "    \"\"\"\n",
    "    Generates positional encoding for sequences.\n",
    "    \"\"\"\n",
    "    positions = np.arange(max_position)[:, np.newaxis]\n",
    "    dims = np.arange(embedding_dim)[np.newaxis, :]\n",
    "\n",
    "    angles = positions / np.power(10000, (2 * (dims // 2)) / embedding_dim)\n",
    "    encoding = np.zeros_like(angles)\n",
    "    encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return tf.cast(encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(input_dim, embedding_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Builds a Transformer encoder block.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim, embedding_dim))\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(inputs, inputs)\n",
    "    attention = Dropout(dropout_rate)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff = Dropout(dropout_rate)(ff)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "    ff = LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "    return Model(inputs, ff, name=\"TransformerEncoder\")\n",
    "\n",
    "def create_transformer(input_dim, embedding_dim, num_heads, ff_dim, num_classes=1, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Builds a Transformer-based classification model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim, embedding_dim))\n",
    "\n",
    "    # Add positional encoding\n",
    "    position_encodings = positional_encoding(input_dim, embedding_dim)\n",
    "    x = inputs + position_encodings\n",
    "\n",
    "    # Transformer encoder block\n",
    "    x = transformer_encoder(input_dim, embedding_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "\n",
    "    # Pooling (reduce sequence to a single vector)\n",
    "    x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "    # Classification head\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs, outputs, name=\"TransformerModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba2ee4b0-2fd8-40d9-87cd-9dbf1eeb215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_texts_testset_transformer(data_dubia, chunk_size, w2v_model, model):\n",
    "    \"\"\"\n",
    "    Prepares and tests the transformer model on all dubia texts, calculating mean predictions for each text.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data_dubia:\n",
    "        if label != -1:\n",
    "            continue  # Process only texts with label -1\n",
    "\n",
    "        words = text.split()\n",
    "        \n",
    "        text_name = words[0]\n",
    "\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            results[text_name] = None  # No valid chunks\n",
    "            continue\n",
    "\n",
    "        text_chunks = [\n",
    "            text_body.split()[i:i + chunk_size]\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert chunks into sequences of embeddings\n",
    "        chunk_embeddings = [\n",
    "            [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "\n",
    "        if not chunk_embeddings:\n",
    "            results[text_name] = None  # No valid embeddings\n",
    "            continue\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "        # Calculate mean prediction for the text\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d419ae-fc2f-4c0a-bb78-4f1b4ccc3274",
   "metadata": {},
   "source": [
    "## Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3bc5200d-01b5-4196-a13f-bfdc192a60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "percentage = 0.2\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lstm_datasets_percentage(data_combined, chunk_size, w2v_model, 0.8, 0.1, 0.1,percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d302d47c-da4f-43cb-9ee0-801d75b542cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"transformer_datasets_{percentage}_{chunk_size}.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1661f997-4049-4240-877c-6bcf55125ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"transformer_datasets_{percentage}_{chunk_size}.pkl\", \"rb\") as f:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb646e9c-b3ea-4ae8-8646-039179ee220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5061/5061 [==============================] - 280s 55ms/step - loss: 0.0800 - accuracy: 0.9691 - val_loss: 0.0791 - val_accuracy: 0.9698\n",
      "633/633 [==============================] - 13s 21ms/step - loss: 0.0803 - accuracy: 0.9691\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = w2v_model.vector_size\n",
    "model = create_transformer(\n",
    "    input_dim=chunk_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_classes=1,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bde558b8-c51d-40cd-8abe-0e2f20279de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.7562\n",
      "Theages: 0.8262\n",
      "Lovers: 0.8636\n",
      "Letters: 0.5299\n",
      "Alcibiades1: 0.8932\n",
      "Alcibiades2: 0.6863\n",
      "Definitions: 0.0004\n",
      "Hipparchus: 0.8368\n",
      "Epinomis: 0.8320\n",
      "Cleitophon: 0.7033\n"
     ]
    }
   ],
   "source": [
    "results = all_texts_testset_lstm(data_dubia, chunk_size, w2v_model, model)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    if mean_score is not None:\n",
    "        print(f\"{text_name}: {mean_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{text_name}: No valid chunks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
