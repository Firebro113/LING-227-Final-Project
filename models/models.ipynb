{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f242caa2-9740-4d3a-a41e-78b057c7f015",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08a1e84-4b6c-49ec-afc8-c6b1f688a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, BatchNormalization, LayerNormalization, Input, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07590c-9b44-4fcb-932c-a18fb8ca69da",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316ab9f7-5e3a-4167-b300-a330c538b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = []\n",
    "\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            data.append((int(parts[0]), parts[1]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a04eb5-24f2-4827-98f7-a4147b6e5910",
   "metadata": {},
   "source": [
    "# Word 2 Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bab59b-7efb-45d5-b239-74f51e0b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greek_W2V(dim, quotes, window, min_count, workers):\n",
    "    '''\n",
    "    Create Word2Vec\n",
    "    '''\n",
    "    w2v_greek = Word2Vec(\n",
    "        sentences=quotes,\n",
    "        vector_size=dim,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers\n",
    "    )\n",
    "\n",
    "    w2v_greek.save(\"greek_word2vec.model\")\n",
    "    return w2v_greek\n",
    "\n",
    "# chunks = []\n",
    "# for label, text in data_combined:\n",
    "#     words = text.split()\n",
    "#     for i in range(0, len(words) - chunk_size + 1):\n",
    "#         chunk = ' '.join(words[i:i + chunk_size])\n",
    "#         chunks.append((label, chunk))\n",
    "\n",
    "# chunks_combined = [chunk.split() for _, chunk in chunks]\n",
    "# dim = 100\n",
    "# window = 5\n",
    "# min_count = 1\n",
    "# workers = 4\n",
    "# w2v_model = greek_W2V(dim, chunks_combined, window, min_count, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a83e-9b8d-4b88-96e2-0ce0f78869b7",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6f298-376b-44e9-b074-f5ab535205ad",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabf07ed-5a5d-4ebc-812c-33b3c5f092a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_chunk(chunk, w2v_model):\n",
    "    \"\"\"\n",
    "    Converts a chunk of text into a fixed-size vector using mean pooling.\n",
    "    \"\"\"\n",
    "    words = chunk.split()\n",
    "    embedding_dim = w2v_model.vector_size\n",
    "    \n",
    "    embeddings = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    \n",
    "    if embeddings:\n",
    "        flattened_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        flattened_embedding = np.zeros(embedding_dim)\n",
    "    \n",
    "    return flattened_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea40602-a130-4725-99ed-75988ccd100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, w2v_model, chunk_size, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Processes text data into sliding window chunks with embeddings,\n",
    "    splits the data into training, validation, and test sets, and randomizes it.\n",
    "    \"\"\"\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            \" \".join(words[i:i + chunk_size])\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        embeddings = [flatten_chunk(chunk, w2v_model) for chunk in chunks]\n",
    "        labels = [label] * len(embeddings)\n",
    "\n",
    "        # Split into train/val/test sets\n",
    "        X_temp, X_test_temp, y_temp, y_test_temp = train_test_split(\n",
    "            embeddings, labels, test_size=test_ratio, random_state=42\n",
    "        )\n",
    "        X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_ratio / (train_ratio + val_ratio), random_state=42\n",
    "        )\n",
    "\n",
    "        # Append to the main dataset\n",
    "        X_train.extend(X_train_temp)\n",
    "        y_train.extend(y_train_temp)\n",
    "        X_val.extend(X_val_temp)\n",
    "        y_val.extend(y_val_temp)\n",
    "        X_test.extend(X_test_temp)\n",
    "        y_test.extend(y_test_temp)\n",
    "\n",
    "    # Shuffle each dataset\n",
    "    train_data = list(zip(X_train, y_train))\n",
    "    val_data = list(zip(X_val, y_val))\n",
    "    test_data = list(zip(X_test, y_test))\n",
    "\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(val_data)\n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    # Unpack shuffled data\n",
    "    X_train, y_train = zip(*train_data)\n",
    "    X_val, y_val = zip(*val_data)\n",
    "    X_test, y_test = zip(*test_data)\n",
    "\n",
    "    return (\n",
    "        np.array(X_train), np.array(y_train),\n",
    "        np.array(X_val), np.array(y_val),\n",
    "        np.array(X_test), np.array(y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56480a7b-f9fc-4d30-982c-bd3b8b7bf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(input_dim):\n",
    "    \"\"\"\n",
    "    Creates a simple feedforward neural network with input dimension specified.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e08c41-7143-4d4e-9470-cbdbd9eb662e",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d62509d-c3e5-494d-89e2-f568a3638a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_combined = load_data(\"./data_clean/combined.txt\")\n",
    "data_dubia = load_data(\"./data_clean/dubia.txt\")\n",
    "\n",
    "# hyperparameters\n",
    "chunk_size = 25\n",
    "dim = 100\n",
    "\n",
    "# load trained word2vec\n",
    "w2v_model = Word2Vec.load(\"greek_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c588ff50-e92e-4f15-a3f3-5f8d039df34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_dataset(data_combined, w2v_model, chunk_size, 0.8, 0.1, 0.1)\n",
    "\n",
    "# Save datasets to a pickle file\n",
    "# with open(f\"nn_datasets_{chunk_size}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)\n",
    "# Load pickled datasets\n",
    "# with open(f\"nn_datasets_{chunk_size}.pkl\", \"rb\") as f:\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f63b26f9-bc98-4c36-8e2a-0c91558e2ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25430/25430 [==============================] - 16s 635us/step - loss: 0.3496 - accuracy: 0.8431 - val_loss: 0.2830 - val_accuracy: 0.8786\n",
      "Epoch 2/10\n",
      "25430/25430 [==============================] - 17s 652us/step - loss: 0.3148 - accuracy: 0.8625 - val_loss: 0.2607 - val_accuracy: 0.8902\n",
      "Epoch 3/10\n",
      "25430/25430 [==============================] - 16s 630us/step - loss: 0.3013 - accuracy: 0.8696 - val_loss: 0.2612 - val_accuracy: 0.8894\n",
      "Epoch 4/10\n",
      "25430/25430 [==============================] - 15s 609us/step - loss: 0.2927 - accuracy: 0.8737 - val_loss: 0.2393 - val_accuracy: 0.8998\n",
      "Epoch 5/10\n",
      "25430/25430 [==============================] - 16s 624us/step - loss: 0.2860 - accuracy: 0.8768 - val_loss: 0.2333 - val_accuracy: 0.9033\n",
      "Epoch 6/10\n",
      "25430/25430 [==============================] - 16s 634us/step - loss: 0.2819 - accuracy: 0.8788 - val_loss: 0.2332 - val_accuracy: 0.9034\n",
      "Epoch 7/10\n",
      "25430/25430 [==============================] - 15s 608us/step - loss: 0.2774 - accuracy: 0.8813 - val_loss: 0.2255 - val_accuracy: 0.9084\n",
      "Epoch 8/10\n",
      "25430/25430 [==============================] - 16s 610us/step - loss: 0.2722 - accuracy: 0.8835 - val_loss: 0.2202 - val_accuracy: 0.9107\n",
      "Epoch 9/10\n",
      "25430/25430 [==============================] - 16s 626us/step - loss: 0.2694 - accuracy: 0.8852 - val_loss: 0.2125 - val_accuracy: 0.9128\n",
      "Epoch 10/10\n",
      "25430/25430 [==============================] - 16s 637us/step - loss: 0.2676 - accuracy: 0.8861 - val_loss: 0.2112 - val_accuracy: 0.9150\n",
      "3180/3180 [==============================] - 1s 317us/step - loss: 0.2133 - accuracy: 0.9139\n"
     ]
    }
   ],
   "source": [
    "# input_dim == dim\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# create model\n",
    "model = create_nn(input_dim)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# evaluate model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6afe4ec-6de2-49b9-9add-9d89e452367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "# model.save_weights(f\"nn_model_weights_{chunk_size}.h5\")\n",
    "\n",
    "# Load model weights\n",
    "# model = create_nn(input_dim)\n",
    "# model.load_weights(f\"nn_model_weights_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22fab7-160e-4b50-9412-cef16cdc883d",
   "metadata": {},
   "source": [
    "## Dubia and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a874d796-723a-4aaa-ada3-0f07145a0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dubia_test_set_fnn(data, chunk_size, w2v_model):\n",
    "    \"\"\"\n",
    "    Prepares a test set for all dubia texts, calculates predictions, and outputs mean scores.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        text_name = words[0]\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue  # Skip if the text is shorter than the chunk size\n",
    "\n",
    "        text_chunks = [\n",
    "            \" \".join(text_body.split()[i:i + chunk_size])\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Generate embeddings for each chunk\n",
    "        chunk_embeddings = [flatten_chunk(chunk, w2v_model) for chunk in text_chunks]\n",
    "\n",
    "        # Convert embeddings to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Calculate the mean prediction\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3685ab5-728a-4a63-979b-7b2ade4f36e6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b231be0-514c-413b-a5c7-fefe321266cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 336us/step\n",
      "108/108 [==============================] - 0s 297us/step\n",
      "75/75 [==============================] - 0s 294us/step\n",
      "530/530 [==============================] - 0s 261us/step\n",
      "321/321 [==============================] - 0s 277us/step\n",
      "130/130 [==============================] - 0s 285us/step\n",
      "60/60 [==============================] - 0s 292us/step\n",
      "70/70 [==============================] - 0s 293us/step\n",
      "197/197 [==============================] - 0s 277us/step\n",
      "48/48 [==============================] - 0s 290us/step\n",
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.85210007\n",
      "Theages: 0.88483202\n",
      "Lovers: 0.90402561\n",
      "Letters: 0.76913136\n",
      "Alcibiades1: 0.90213656\n",
      "Alcibiades2: 0.82479775\n",
      "Definitions: 0.32285777\n",
      "Hipparchus: 0.84649581\n",
      "Epinomis: 0.80118346\n",
      "Cleitophon: 0.82976156\n"
     ]
    }
   ],
   "source": [
    "# Test on Data Dubia\n",
    "results = dubia_test_set_fnn(data_dubia, chunk_size, w2v_model)\n",
    "\n",
    "# Print the mean scores for each text\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    print(f\"{text_name}: {mean_score:.8f}\")\n",
    "\n",
    "# Append test set if needed\n",
    "results[\"Test Set\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c986a6-38d1-4f19-ada1-f94721cf63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# with open(\"fnn_results.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tabs, file)\n",
    "\n",
    "# Load results\n",
    "# with open(\"fnn_results.pkl\", \"rb\") as file:\n",
    "#     tabs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345e9ff-b1bb-42e2-8116-f133f3047d7b",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df52fb2-7863-4e08-b5a1-c18f86e4d4c2",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d11725-1f52-4ced-b3cf-2a91986db927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_datasets(data, chunk_size, w2v_model, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Prepares datasets for an LSTM model by processing text into sequences of embeddings and splitting into train/val/test.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            continue\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            words[i:i + chunk_size]\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert each chunk into a sequence of embeddings\n",
    "        for chunk in chunks:\n",
    "            embeddings = [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            X.append(embeddings)\n",
    "            y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split into train, val, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(val_ratio + test_ratio), random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3adaa756-439b-43f0-9234-1cb5e6fe3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_datasets_percentage(data, chunk_size, w2v_model, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, sample_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Prepares datasets for an LSTM model by processing text into sequences of embeddings and splitting into train/val/test.\n",
    "    Samples a percentage of chunks from each text to reduce dataset size.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label, text in data:\n",
    "        words = text.split()\n",
    "        num_chunks = len(words) - chunk_size + 1\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        chunks = [\n",
    "            words[i:i + chunk_size]\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Sample a percentage of the chunks\n",
    "        sampled_chunks = sample(chunks, int(len(chunks) * sample_ratio))\n",
    "\n",
    "        # Convert each chunk into a sequence of embeddings\n",
    "        for chunk in sampled_chunks:\n",
    "            embeddings = [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            X.append(embeddings)\n",
    "            y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split into train, val, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(val_ratio + test_ratio), random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb28c85-470a-450c-8beb-a4f6b195b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(input_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Creates a neural network with an LSTM layer to process sequential embeddings.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, input_shape=(input_dim, embedding_dim), return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b799b-a00a-467c-940e-ec29c550780a",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f99d2ad-58c1-4358-8ea0-b7477009a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_combined = load_data(\"./data_clean/combined.txt\")\n",
    "data_dubia = load_data(\"./data_clean/dubia.txt\")\n",
    "\n",
    "# hyperparameters\n",
    "chunk_size = 25\n",
    "dim = 100\n",
    "percentage = 0.05\n",
    "\n",
    "# load trained word2vec\n",
    "w2v_model = Word2Vec.load(\"greek_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c69137-8d2e-4cd0-8498-08c2e8003cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lstm_datasets_percentage(data_combined, chunk_size, w2v_model, 0.8, 0.1, 0.1, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91ae3636-8c30-4014-a6f1-dee82aced6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to a pickle file\n",
    "# with open(f\"lstm_datasets_{percentage}_{chunk_size}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)\n",
    "\n",
    "# Load datasets\n",
    "# with open(f\"lstm_datasets_{percentage}_{chunk_size}.pkl\", \"rb\") as f:\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ff1528-535f-4e0c-9895-39744e9c04f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1271/1271 [==============================] - 12s 9ms/step - loss: 0.3729 - accuracy: 0.8318 - val_loss: 0.2362 - val_accuracy: 0.9032\n",
      "Epoch 2/5\n",
      "1271/1271 [==============================] - 12s 9ms/step - loss: 0.2162 - accuracy: 0.9136 - val_loss: 0.2055 - val_accuracy: 0.9176\n",
      "Epoch 3/5\n",
      "1271/1271 [==============================] - 11s 9ms/step - loss: 0.1678 - accuracy: 0.9360 - val_loss: 0.1838 - val_accuracy: 0.9300\n",
      "Epoch 4/5\n",
      "1271/1271 [==============================] - 11s 9ms/step - loss: 0.1290 - accuracy: 0.9513 - val_loss: 0.2090 - val_accuracy: 0.9243\n",
      "Epoch 5/5\n",
      "1271/1271 [==============================] - 11s 9ms/step - loss: 0.0989 - accuracy: 0.9640 - val_loss: 0.1729 - val_accuracy: 0.9382\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.1760 - accuracy: 0.9394\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = create_lstm(chunk_size, w2v_model.vector_size)\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# evaluate model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "239857ad-4b87-4219-80a5-89dc44ea8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# model.save_weights(f\"lstm_model_weights_{percentage}_{chunk_size}.h5\")\n",
    "\n",
    "# Load weights\n",
    "# model = create_lstm(chunk_size, w2v_model.vector_size)\n",
    "\n",
    "# model.load_weights(f\"lstm_model_weights_{percentage}_{chunk_size}.h5\")\n",
    "# print(f\"lstm_model_weights_{percentage}_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746e9f2-91fb-4518-863e-d7f97927fb7e",
   "metadata": {},
   "source": [
    "## Dubia Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ca4497-2df9-4ce3-8e6f-da7ca8986642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dubia_test_set_lstm(data, chunk_size, w2v_model, model):\n",
    "    \"\"\"\n",
    "    Prepares and tests the LSTM model on all dubia texts, calculating mean predictions for each text.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data:\n",
    "\n",
    "        words = text.split()\n",
    "        \n",
    "        text_name = words[0]\n",
    "\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "\n",
    "        text_chunks = [\n",
    "            text_body.split()[i:i + chunk_size]\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert chunks into sequences of embeddings\n",
    "        chunk_embeddings = [\n",
    "            [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "\n",
    "        if not chunk_embeddings:\n",
    "            results[text_name] = None  # No valid embeddings\n",
    "            continue\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "        # Calculate mean prediction for the text\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "428d0564-ebc3-4d18-b9f2-b56b0aece94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.8358\n",
      "Theages: 0.8444\n",
      "Lovers: 0.8806\n",
      "Letters: 0.6888\n",
      "Alcibiades1: 0.8907\n",
      "Alcibiades2: 0.7954\n",
      "Definitions: 0.2262\n",
      "Hipparchus: 0.8412\n",
      "Epinomis: 0.8380\n",
      "Cleitophon: 0.8335\n"
     ]
    }
   ],
   "source": [
    "results = dubia_test_set_lstm(data_dubia, chunk_size, w2v_model, model)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    print(f\"{text_name}: {mean_score:.4f}\")\n",
    "\n",
    "# If needed\n",
    "results[\"Test Set\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c947765c-effa-42eb-9431-147b0c4a7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# with open(\"lstm_results.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tabs, file)\n",
    "\n",
    "# Load results\n",
    "# with open(\"lstm_results.pkl\", \"rb\") as file:\n",
    "#     tabs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64982a07-7338-42e2-9233-aeb8720a0e52",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548078d-83ee-48f0-a711-c1944c6e82ef",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48ba914f-a9e6-49e2-8337-1cff73a9ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_position, embedding_dim):\n",
    "    \"\"\"\n",
    "    Generates positional encoding for sequences.\n",
    "    \"\"\"\n",
    "    positions = np.arange(max_position)[:, np.newaxis]\n",
    "    dims = np.arange(embedding_dim)[np.newaxis, :]\n",
    "\n",
    "    angles = positions / np.power(10000, (2 * (dims // 2)) / embedding_dim)\n",
    "    encoding = np.zeros_like(angles)\n",
    "    encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return tf.cast(encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(input_dim, embedding_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Builds a Transformer encoder block.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim, embedding_dim))\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(inputs, inputs)\n",
    "    attention = Dropout(dropout_rate)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff = Dropout(dropout_rate)(ff)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "    ff = LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "    return Model(inputs, ff, name=\"TransformerEncoder\")\n",
    "\n",
    "def create_transformer(input_dim, embedding_dim, num_heads, ff_dim, num_classes=1, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Builds a Transformer-based classification model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim, embedding_dim))\n",
    "\n",
    "    # Add positional encoding\n",
    "    position_encodings = positional_encoding(input_dim, embedding_dim)\n",
    "    x = inputs + position_encodings\n",
    "\n",
    "    # Transformer encoder block\n",
    "    x = transformer_encoder(input_dim, embedding_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "\n",
    "    # Pooling (reduce sequence to a single vector)\n",
    "    x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "    # Classification head\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs, outputs, name=\"TransformerModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f964c00-b5c4-4ad7-82f4-fe9c9a38d7ce",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55cf45d4-dd4b-4bba-af2f-64921ed3c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_combined = load_data(\"./data_clean/combined.txt\")\n",
    "data_dubia = load_data(\"./data_clean/dubia.txt\")\n",
    "\n",
    "# hyperparameters\n",
    "chunk_size = 25\n",
    "dim = 100\n",
    "percentage = 0.05\n",
    "\n",
    "# load trained word2vec\n",
    "w2v_model = Word2Vec.load(\"greek_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af9dd5ec-fbbb-49d9-a26a-47fd984c93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = lstm_datasets_percentage(data_combined, chunk_size, w2v_model, 0.8, 0.1, 0.1,percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f91dcfe-136a-433c-8dc4-397df0eb4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data sets\n",
    "# with open(f\"transformer_datasets_{percentage}_{chunk_size}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_train, y_train, X_val, y_val, X_test, y_test), f)\n",
    "\n",
    "# Load Data sets\n",
    "# with open(f\"transformer_datasets_{percentage}_{chunk_size}.pkl\", \"rb\") as f:\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cef69b19-7829-4510-942d-9a34d38b1636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1271/1271 [==============================] - 12s 9ms/step - loss: 0.3338 - accuracy: 0.8499 - val_loss: 0.2682 - val_accuracy: 0.8885\n",
      "Epoch 2/5\n",
      "1271/1271 [==============================] - 13s 10ms/step - loss: 0.2520 - accuracy: 0.8933 - val_loss: 0.2431 - val_accuracy: 0.9001\n",
      "Epoch 3/5\n",
      "1271/1271 [==============================] - 13s 10ms/step - loss: 0.2286 - accuracy: 0.9054 - val_loss: 0.2678 - val_accuracy: 0.8924\n",
      "Epoch 4/5\n",
      "1271/1271 [==============================] - 13s 10ms/step - loss: 0.2050 - accuracy: 0.9150 - val_loss: 0.2354 - val_accuracy: 0.9038\n",
      "Epoch 5/5\n",
      "1271/1271 [==============================] - 13s 10ms/step - loss: 0.1890 - accuracy: 0.9223 - val_loss: 0.2384 - val_accuracy: 0.9018\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.2381 - accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "# embedding dimension\n",
    "embedding_dim = w2v_model.vector_size\n",
    "\n",
    "# create model\n",
    "model = create_transformer(\n",
    "    input_dim=chunk_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_classes=1,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "519bfebd-3226-4fde-b22f-674f0ef2511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# model.save_weights(f\"transformer_model_weights_{percentage}_{chunk_size}.h5\")\n",
    "\n",
    "# Load weights\n",
    "# model = create_transformer(chunk_size, w2v_model.vector_size, 4, 128)\n",
    "\n",
    "# model.load_weights(f\"transformer_model_weights_{percentage}_{chunk_size}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a552c-6a5d-436e-a2cc-18ae802c47df",
   "metadata": {},
   "source": [
    "## Dubia Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba2ee4b0-2fd8-40d9-87cd-9dbf1eeb215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dubia_test_set_transformer(data_dubia, chunk_size, w2v_model, model):\n",
    "    \"\"\"\n",
    "    Prepares and tests the transformer model on all dubia texts, calculating mean predictions for each text.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for label, text in data_dubia:\n",
    "        if label != -1:\n",
    "            continue  # Process only texts with label -1\n",
    "\n",
    "        words = text.split()\n",
    "        \n",
    "        text_name = words[0]\n",
    "\n",
    "        text_body = \" \".join(words[1:])\n",
    "\n",
    "        # Generate sliding window chunks\n",
    "        num_chunks = len(text_body.split()) - chunk_size + 1\n",
    "        if num_chunks <= 0:\n",
    "            results[text_name] = None  # No valid chunks\n",
    "            continue\n",
    "\n",
    "        text_chunks = [\n",
    "            text_body.split()[i:i + chunk_size]\n",
    "            for i in range(1, num_chunks)\n",
    "        ]\n",
    "\n",
    "        # Convert chunks into sequences of embeddings\n",
    "        chunk_embeddings = [\n",
    "            [\n",
    "                w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size)\n",
    "                for word in chunk\n",
    "            ]\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "\n",
    "        if not chunk_embeddings:\n",
    "            results[text_name] = None  # No valid embeddings\n",
    "            continue\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        X_test = np.array(chunk_embeddings)\n",
    "\n",
    "        # Get predictions for the embeddings\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "        # Calculate mean prediction for the text\n",
    "        mean_score = np.mean(predictions)\n",
    "        results[text_name] = mean_score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8801d1ef-25c3-4499-8b98-b97256a99107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Predictions for Dubia Texts:\n",
      "Minos: 0.7783\n",
      "Theages: 0.8060\n",
      "Lovers: 0.8011\n",
      "Letters: 0.6179\n",
      "Alcibiades1: 0.8512\n",
      "Alcibiades2: 0.7306\n",
      "Definitions: 0.2001\n",
      "Hipparchus: 0.7749\n",
      "Epinomis: 0.7575\n",
      "Cleitophon: 0.7403\n"
     ]
    }
   ],
   "source": [
    "results = dubia_test_set_transformer(data_dubia, chunk_size, w2v_model, model)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nMean Predictions for Dubia Texts:\")\n",
    "for text_name, mean_score in results.items():\n",
    "    print(f\"{text_name}: {mean_score:.4f}\")\n",
    "\n",
    "# if needed\n",
    "results[\"Test Set\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb81bc7d-37d6-4dab-8d69-f575368a3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# with open(\"transformer_results.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tabs, file)\n",
    "\n",
    "# Load results\n",
    "# with open(\"transformer_results.pkl\", \"rb\") as file:\n",
    "#     tabs = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
